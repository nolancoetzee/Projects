{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "https://colab.research.google.com/drive/1mq6dEJ4O6YK92DnpxlKlcW50PgaMdc4P?usp=sharing  CLICK HERE TO VIEW ALL OUTPUTS!!! - Nolan "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Vm2IcudFPrj"
      },
      "outputs": [],
      "source": [
        "!pip install scikeras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o26cNcz1FKKi"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import backend as K\n",
        "# import libraries\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout\n",
        "from tensorflow.random import set_seed\n",
        "from random import seed\n",
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6VRRdofg08HJ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = '/content/drive/MyDrive/Infor648/Data/Data3.csv'\n",
        "df = pd.read_csv(file_path, encoding='cp1252')  # Windows-1252\n",
        "print(f\"Loaded {len(df)} entries\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gOypMf6Y6TdI"
      },
      "outputs": [],
      "source": [
        "df_sub = df[[\n",
        "    \"Data\"\n",
        "]]\n",
        "\n",
        "# Display the first few rows of the subset DataFrame\n",
        "df_sub.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vPgMt1UaMiup"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import pandas as pd\n",
        "\n",
        "# Combine all data entries into one string\n",
        "combined_text = ' '.join(df_sub['Data'])\n",
        "\n",
        "# Tokenize combined text\n",
        "language = []\n",
        "words = re.split(r'[ ]\\s*', combined_text)\n",
        "language.extend(words)\n",
        "\n",
        "# Dictionary to hold unique words and their assigned numbers\n",
        "tokens = {}\n",
        "counter = 1\n",
        "\n",
        "tokens[\".\"] = 377\n",
        "\n",
        "# Assign tokens to each unique word\n",
        "for word in language:\n",
        "    if word not in tokens:\n",
        "        tokens[word] = counter\n",
        "        counter += 1\n",
        "\n",
        "# Function to convert text to tokens, splitting periods\n",
        "def phrase_converter(text, tokens):\n",
        "    # Add space before periods to match initial tokenization\n",
        "    text = re.sub(r'(\\w)\\.', r'\\1 .', text)\n",
        "    words = re.split(r'[ ]\\s*', text)\n",
        "    return ' '.join(str(tokens.get(word, 0)) for word in words)\n",
        "\n",
        "# Apply the conversion to each entry\n",
        "df_sub['sequence'] = df_sub['Data'].apply(lambda x: phrase_converter(x, tokens))\n",
        "\n",
        "\n",
        "# Print token encoding summary\n",
        "print(\"Token Encoding:\")\n",
        "print(df_sub['sequence'].value_counts())\n",
        "\n",
        "# Function to create [word, token] pairs for each sequence\n",
        "def create_word_token_pairs(sequence, token_dict):  # Renamed to avoid confusion\n",
        "    # Reverse the tokens dictionary for lookup\n",
        "    reverse_tokens = {v: k for k, v in token_dict.items()}  # token_dict should be a dict here\n",
        "    # Split the sequence into token IDs\n",
        "    token_ids = sequence.split()\n",
        "    # Create [word, token] pairs\n",
        "    pairs = [[reverse_tokens.get(int(token), \"UNKNOWN\"), int(token)] for token in token_ids]\n",
        "    return pairs\n",
        "\n",
        "# Add the [word, token] pairs to a new column\n",
        "df_sub['dictionary'] = df_sub['sequence'].apply(lambda x: create_word_token_pairs(x, tokens))\n",
        "\n",
        "# Print the updated DataFrame\n",
        "print(\"\\nUpdated DataFrame with 'dictionary':\")\n",
        "print(df_sub[['Data', 'sequence', 'dictionary']])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ecrs_P3iat6L"
      },
      "outputs": [],
      "source": [
        "print(\"Washington token:\", tokens.get(\"Washington\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6oCLtmCEqmF0"
      },
      "outputs": [],
      "source": [
        "df_sub.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Rs2hwvhj_vD"
      },
      "outputs": [],
      "source": [
        "# Apply the conversion to each entry\n",
        "df_sub['sequence'] = df_sub['Data'].apply(lambda x: phrase_converter(x, tokens))\n",
        "\n",
        "# Prepare X and y with a window size of 2\n",
        "X = []\n",
        "y = []\n",
        "window_size = 7\n",
        "\n",
        "for sequence in df_sub['sequence']:\n",
        "    tokens = sequence.split()  # Split sequence into tokens\n",
        "    tokens = [int(token) for token in tokens]  # Convert tokens to integers\n",
        "    # Ensure there are enough tokens for window_size + 1 (2 inputs + 1 output)\n",
        "    for i in range(len(tokens) - window_size):\n",
        "        X.append(tokens[i:i + window_size])  # Two consecutive tokens\n",
        "        y.append(tokens[i + window_size])    # Next token\n",
        "\n",
        "X = np.array(X)  # Shape: (num_samples, 2)\n",
        "y = np.array(y)  # Shape: (num_samples,)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "viyTsfemxYop"
      },
      "outputs": [],
      "source": [
        "#Split the dataset into training and testing sets test_size using 0.3: 70% training and 30% testing\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
        "\n",
        "#Display the shapes of the resulting datasets\n",
        "print(\"Training Features Shape:\", X_train.shape)\n",
        "print(\"Testing Features Shape:\", X_test.shape)\n",
        "print(\"Training Target Shape:\", y_train.shape)\n",
        "print(\"Testing Target Shape:\", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "L0z13sgsO18Y"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjf5CZ5WKfuq"
      },
      "source": [
        "Nueral Network\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "kjnyhOA1O5eE"
      },
      "outputs": [],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZrbYDnJPaVD3"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSJJYfkIeagz"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Activation Functions (unchanged)\n",
        "def relu(x):\n",
        "    return np.maximum(0, x)\n",
        "\n",
        "def relu_derivative(x):\n",
        "    return (x > 0).astype(float)\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
        "    return e_x / e_x.sum(axis=1, keepdims=True)\n",
        "\n",
        "# SimpleNeuralNetwork class (unchanged)\n",
        "class SimpleNeuralNetwork:\n",
        "    def __init__(self, input_size, hidden_size=128, output_size=377, learning_rate=0.01):\n",
        "        self.W1 = np.random.randn(input_size, hidden_size) * np.sqrt(2.0 / input_size)\n",
        "        self.b1 = np.zeros((1, hidden_size))\n",
        "        self.W2 = np.random.randn(hidden_size, output_size) * np.sqrt(2.0 / hidden_size)\n",
        "        self.b2 = np.zeros((1, output_size))\n",
        "        self.lr = learning_rate\n",
        "        self.loss_history = []\n",
        "\n",
        "    def forward(self, X):\n",
        "        self.z1 = np.dot(X, self.W1) + self.b1\n",
        "        self.a1 = relu(self.z1)\n",
        "        self.z2 = np.dot(self.a1, self.W2) + self.b2\n",
        "        self.a2 = softmax(self.z2)\n",
        "        return self.a2\n",
        "\n",
        "    def backward(self, X, y, output):\n",
        "        n_samples = X.shape[0]\n",
        "        y_adjusted = y - 1\n",
        "        y_one_hot = np.zeros((n_samples, self.W2.shape[1]))\n",
        "        y_one_hot[np.arange(n_samples), y_adjusted] = 1\n",
        "\n",
        "        self.error = output - y_one_hot\n",
        "        self.delta2 = self.error\n",
        "\n",
        "        self.delta1 = np.dot(self.delta2, self.W2.T) * relu_derivative(self.a1)\n",
        "\n",
        "        self.dW2 = np.dot(self.a1.T, self.delta2) / n_samples + 0.01 * self.W2\n",
        "        self.db2 = np.sum(self.delta2, axis=0, keepdims=True) / n_samples\n",
        "        self.dW1 = np.dot(X.T, self.delta1) / n_samples + 0.01 * self.W1\n",
        "        self.db1 = np.sum(self.delta1, axis=0, keepdims=True) / n_samples\n",
        "\n",
        "        self.W2 -= self.lr * self.dW2\n",
        "        self.b2 -= self.lr * self.db2\n",
        "        self.W1 -= self.lr * self.dW1\n",
        "        self.b1 -= self.lr * self.db1\n",
        "\n",
        "    def train(self, X, y, epochs=100, batch_size=32):\n",
        "        for epoch in range(epochs):\n",
        "            indices = np.random.permutation(X.shape[0])\n",
        "            X_shuffled = X[indices]\n",
        "            y_shuffled = y[indices]\n",
        "\n",
        "            for i in range(0, X.shape[0], batch_size):\n",
        "                batch_X = X_shuffled[i:i + batch_size]\n",
        "                batch_y = y_shuffled[i:i + batch_size]\n",
        "                output = self.forward(batch_X)\n",
        "                self.backward(batch_X, batch_y, output)\n",
        "\n",
        "            output_full = self.forward(X)\n",
        "            loss = -np.mean(np.log(output_full[np.arange(X.shape[0]), y - 1] + 1e-10))\n",
        "            self.loss_history.append(loss)\n",
        "            print(f\"Epoch {epoch + 1}/{epochs}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        output = self.forward(X)\n",
        "        return np.argmax(output, axis=1) + 1\n",
        "\n",
        "    def plot_loss(self):\n",
        "        plt.plot(range(1, len(self.loss_history) + 1), self.loss_history, label='Loss')\n",
        "        plt.xlabel('Epoch')\n",
        "        plt.ylabel('Loss')\n",
        "        plt.title('Loss Over Epochs')\n",
        "        plt.legend()\n",
        "        plt.grid(True)\n",
        "        plt.show()\n",
        "\n",
        "# Your data prep (unchanged)\n",
        "# Pad sequences with [0, 0, 1], [0, 1, 2], etc.\n",
        "df_sub['sequence'] = [f'{\"0 \" * (window_size - 1)}{seq}' for seq in df_sub['sequence']]\n",
        "\n",
        "# Prepare X and y with window size 3\n",
        "X = []\n",
        "y = []\n",
        "window_size = 7\n",
        "for sequence in df_sub['sequence']:\n",
        "    tokens = sequence.split()\n",
        "    tokens = [int(token) for token in tokens]\n",
        "    for i in range(len(tokens) - window_size):\n",
        "        X.append(tokens[i:i + window_size])\n",
        "        y.append(tokens[i + window_size])\n",
        "\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# One-hot encode X\n",
        "vocab_size = 377\n",
        "num_samples = X.shape[0]\n",
        "X_one_hot = np.zeros((num_samples, window_size * vocab_size))\n",
        "for i in range(window_size):\n",
        "    X_one_hot[np.arange(num_samples), (i * vocab_size) + X[:, i] - 1] = 1\n",
        "\n",
        "# Train the model\n",
        "nn = SimpleNeuralNetwork(input_size=window_size * vocab_size, hidden_size=64, output_size=377, learning_rate=0.1)\n",
        "nn.train(X_one_hot, y, epochs=100, batch_size=32)\n",
        "\n",
        "# Plot the loss\n",
        "nn.plot_loss()\n",
        "\n",
        "# Generation loop with last_prediction penalty\n",
        "max_tokens = 20\n",
        "end_token = 377  # \".\" is 377\n",
        "initial_tokens = [1, 2, 3, 4, 5, 6, 7]  # George Washington ?\n",
        "generated_tokens = initial_tokens.copy()\n",
        "current_window = initial_tokens.copy()\n",
        "last_prediction = None\n",
        "\n",
        "for _ in range(max_tokens):\n",
        "    test_input = np.zeros((1, window_size * vocab_size))\n",
        "    for i, token in enumerate(current_window):\n",
        "        test_input[0, i * vocab_size + token - 1] = 1\n",
        "    probs = nn.forward(test_input)  # Get raw probabilities\n",
        "    # Penalize 158 if it was the last prediction\n",
        "    if last_prediction == end_token:\n",
        "        probs[0, end_token - 1] *= 0.5  # Reduce probability of 158 by 90%\n",
        "    prediction = np.argmax(probs, axis=1)[0] + 1  # Pick highest after penalty\n",
        "    generated_tokens.append(prediction)\n",
        "    current_window.pop(0)\n",
        "    current_window.append(prediction)\n",
        "    last_prediction = prediction  # Update last prediction\n",
        "    if prediction == end_token:\n",
        "        break\n",
        "# Function to extract reverse tokens from df_sub['dictionary']\n",
        "def get_reverse_tokens_from_dictionary(df_dictionary):\n",
        "    reverse_tokens = {}\n",
        "    for sequence_pairs in df_dictionary:\n",
        "        for word, token in sequence_pairs:\n",
        "            reverse_tokens[token] = word  # Map token ID to word\n",
        "    return reverse_tokens\n",
        "\n",
        "# Function to convert generated tokens to words using df_sub['dictionary'] (unchanged)\n",
        "def tokens_to_words_from_dict(generated_tokens, df_dictionary):\n",
        "    if isinstance(generated_tokens, list):\n",
        "        token_str = ' '.join(map(str, generated_tokens))\n",
        "    else:\n",
        "        token_str = generated_tokens\n",
        "    token_ids = token_str.split()\n",
        "    reverse_tokens = get_reverse_tokens_from_dictionary(df_dictionary)\n",
        "    return ' '.join(reverse_tokens.get(int(token), \"UNKNOWN\") for token in token_ids)\n",
        "\n",
        "# Convert to words (assuming your token-to-word function is defined)\n",
        "word_sequence = tokens_to_words_from_dict(generated_tokens, df_sub['dictionary'])\n",
        "print(f\"Generated sequence: {generated_tokens}\")\n",
        "print(f\"Word sequence: {word_sequence}\")\n",
        "\n",
        "# Your existing debug code (unchanged)\n",
        "probs = nn.forward(test_input)\n",
        "top_5_indices = np.argsort(probs[0])[::-1][:5] + 1\n",
        "top_5_probs = probs[0, top_5_indices - 1]\n",
        "print(\"Top 5 predicted tokens and probabilities:\")\n",
        "for idx, prob in zip(top_5_indices, top_5_probs):\n",
        "    print(f\"Token {idx}: {prob:.4f}\")\n",
        "\n",
        "from collections import Counter\n",
        "targets = []\n",
        "for i in range(num_samples):\n",
        "    if np.array_equal(X[i], [1, 2, 3]):\n",
        "        targets.append(y[i])\n",
        "print(\"Tokens following [1, 2, 3]:\", Counter(targets))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rwqcKphfGPO"
      },
      "outputs": [],
      "source": [
        "# Parameters\n",
        "window_size = 7\n",
        "vocab_size = 377\n",
        "max_tokens = 20\n",
        "end_token = 376\n",
        "\n",
        "# Initial sequence\n",
        "initial_tokens = [1, 2, 3, 5, 6, 7]  # e.g., \"George Washington ?\"\n",
        "generated_tokens = initial_tokens.copy()  # Start with initial tokens\n",
        "current_window = initial_tokens.copy()\n",
        "last_prediction = None  # Track the last predicted token\n",
        "\n",
        "# Generation loop with repetition penalty\n",
        "for _ in range(max_tokens):\n",
        "    # Prepare one-hot encoded input\n",
        "    test_input = np.zeros((1, window_size * vocab_size))\n",
        "    for i, token in enumerate(current_window):\n",
        "        test_input[0, i * vocab_size + token - 1] = 1  # One-hot encode\n",
        "\n",
        "    # Get probabilities and apply penalty if last was 158\n",
        "    probs = nn.forward(test_input)  # Shape: [1, 158]\n",
        "    if last_prediction == 377:  # Penalize \".\" if it was last\n",
        "        probs[0, 377 - 1] *= 0.001  # Reduce probability of 158 by 90%\n",
        "    prediction = np.argmax(probs, axis=1)[0] + 1  # Get scalar prediction after penalty\n",
        "\n",
        "    # Append to sequence\n",
        "    generated_tokens.append(prediction)\n",
        "\n",
        "    # Shift window: drop leftmost, add new prediction\n",
        "    current_window.pop(0)\n",
        "    current_window.append(prediction)\n",
        "\n",
        "    # Update last prediction\n",
        "    last_prediction = prediction\n",
        "\n",
        "    # Stop if end_token is predicted\n",
        "    if prediction == end_token:\n",
        "        break\n",
        "\n",
        "# Function to extract reverse tokens from df_sub['dictionary'] (unchanged)\n",
        "def get_reverse_tokens_from_dictionary(df_dictionary):\n",
        "    reverse_tokens = {}\n",
        "    for sequence_pairs in df_dictionary:\n",
        "        for word, token in sequence_pairs:\n",
        "            reverse_tokens[token] = word  # Map token ID to word\n",
        "    return reverse_tokens\n",
        "\n",
        "# Function to convert generated tokens to words using df_sub['dictionary'] (unchanged)\n",
        "def tokens_to_words_from_dict(generated_tokens, df_dictionary):\n",
        "    if isinstance(generated_tokens, list):\n",
        "        token_str = ' '.join(map(str, generated_tokens))\n",
        "    else:\n",
        "        token_str = generated_tokens\n",
        "    token_ids = token_str.split()\n",
        "    reverse_tokens = get_reverse_tokens_from_dictionary(df_dictionary)\n",
        "    return ' '.join(reverse_tokens.get(int(token), \"UNKNOWN\") for token in token_ids)\n",
        "\n",
        "# Convert the generated sequence to words\n",
        "word_sequence = tokens_to_words_from_dict(generated_tokens, df_sub['dictionary'])\n",
        "\n",
        "# Print results\n",
        "print(f\"Generated sequence: {generated_tokens}\")\n",
        "print(f\"Word sequence: {word_sequence}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "985v_rUSRhPo"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# Parameters\n",
        "window_size = 7\n",
        "vocab_size = 377  # Total number of unique tokens\n",
        "max_tokens = 20\n",
        "end_token = 378  # Period token to stop generation\n",
        "\n",
        "# Assume nn is your trained neural network model (e.g., an instance of your class)\n",
        "\n",
        "# Function to extract reverse tokens from df_sub['dictionary']\n",
        "def get_reverse_tokens_from_dictionary(df_dictionary):\n",
        "    reverse_tokens = {}\n",
        "    for sequence_pairs in df_dictionary:\n",
        "        for word, token in sequence_pairs:\n",
        "            reverse_tokens[token] = word  # Map token ID to word\n",
        "    return reverse_tokens\n",
        "\n",
        "# Function to convert tokens to words using df_sub['dictionary']\n",
        "def tokens_to_words_from_dict(generated_tokens, df_dictionary):\n",
        "    if isinstance(generated_tokens, list):\n",
        "        token_str = ' '.join(map(str, generated_tokens))\n",
        "    else:\n",
        "        token_str = generated_tokens\n",
        "    token_ids = token_str.split()\n",
        "    reverse_tokens = get_reverse_tokens_from_dictionary(df_dictionary)\n",
        "    return ' '.join(reverse_tokens.get(int(token), \"UNKNOWN\") for token in token_ids)\n",
        "\n",
        "# Function to get token ID from a word using df_sub['dictionary']\n",
        "def word_to_token(word, df_dictionary):\n",
        "    reverse_tokens = get_reverse_tokens_from_dictionary(df_dictionary)\n",
        "    # Create forward mapping: word -> token\n",
        "    forward_tokens = {v: k for k, v in reverse_tokens.items()}\n",
        "    return forward_tokens.get(word, 0)  # Return 0 if word not found\n",
        "\n",
        "# Prompt user for input and generate sequence\n",
        "def generate_sequence_from_word(nn, df_dictionary):\n",
        "    # Get input from user\n",
        "    input_word = input(\"Enter a word to start the sequence: \").strip()\n",
        "\n",
        "    # Convert input word to token\n",
        "    start_token = word_to_token(input_word, df_dictionary)\n",
        "    if start_token == 0:\n",
        "        print(f\"Word '{input_word}' not found in dictionary. Using token 0.\")\n",
        "\n",
        "    # Initialize sequence with the single token (padded with zeros for window)\n",
        "    initial_tokens = [0] * (window_size - 1) + [start_token]  # Pad with zeros to fill window\n",
        "    generated_tokens = [start_token]  # Start sequence with the input token\n",
        "    current_window = initial_tokens.copy()  # Full window for prediction\n",
        "    last_prediction = None\n",
        "\n",
        "    # Generation loop\n",
        "    for _ in range(max_tokens):\n",
        "        # Prepare one-hot encoded input\n",
        "        test_input = np.zeros((1, window_size * vocab_size))\n",
        "        for i, token in enumerate(current_window):\n",
        "            test_input[0, i * vocab_size + token - 1] = 1  # One-hot encode (adjust for 1-based tokens)\n",
        "\n",
        "        # Get probabilities and apply penalty if last was 377 (period)\n",
        "        probs = nn.forward(test_input)  # Shape: [1, vocab_size]\n",
        "        if last_prediction == 377:  # Penalize \".\" if it was last\n",
        "            probs[0, 377 - 1] *= 0.0001  # Reduce probability of period by 99.9%\n",
        "        prediction = np.argmax(probs, axis=1)[0] + 1  # Get scalar prediction (1-based)\n",
        "\n",
        "                # Get probabilities and apply penalty if last was 377 (period)\n",
        "        probs = nn.forward(test_input)  # Shape: [1, vocab_size]\n",
        "        if last_prediction == 376:  # Penalize \".\" if it was last\n",
        "            probs[0, 376 - 1] *= 0.001  # Reduce probability of period by 99.9%\n",
        "        prediction = np.argmax(probs, axis=1)[0] + 1  # Get scalar prediction (1-based)\n",
        "\n",
        "        # Append to sequence\n",
        "        generated_tokens.append(prediction)\n",
        "\n",
        "        # Shift window: drop leftmost, add new prediction\n",
        "        current_window.pop(0)\n",
        "        current_window.append(prediction)\n",
        "\n",
        "        # Update last prediction\n",
        "        last_prediction = prediction\n",
        "\n",
        "        # Stop if end_token is predicted\n",
        "        if prediction == end_token:\n",
        "            break\n",
        "\n",
        "    # Convert tokens to words\n",
        "    word_sequence = tokens_to_words_from_dict(generated_tokens, df_dictionary)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"Generated sequence: {generated_tokens}\")\n",
        "    print(f\"Word sequence: {word_sequence}\")\n",
        "\n",
        "# Example usage (assuming nn and df_sub are defined)\n",
        "# Replace 'nn' with your actual neural network instance\n",
        "generate_sequence_from_word(nn, df_sub['dictionary'])"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
